from __future__ import annotations

from decimal import Decimal

from django.db import connection
from django.db.models import Avg
from django.utils import timezone

from nbms_app.indicator_methods.base import BaseIndicatorMethod, MethodResult
from nbms_app.models import (
    BinaryIndicatorResponse,
    DatasetRelease,
    EcosystemGoldSummary,
    EcosystemGoldDimension,
    EicatCategory,
    IndicatorDataPoint,
    IndicatorDataSeries,
    IndicatorFrameworkIndicatorLink,
    IndicatorValueType,
    IASGoldSummary,
    LifecycleStatus,
    MonitoringProgrammeRun,
    SeicatCategory,
    SensitivityLevel,
    SpatialFeature,
    SpatialLayer,
    SpatialUnit,
    SpatialUnitType,
    TaxonGoldSummary,
)
from nbms_app.spatial_fields import GIS_ENABLED


def _resolve_run_year(context):
    value = context.params.get("year")
    try:
        return int(value)
    except (TypeError, ValueError):
        return timezone.now().year


def _upsert_registry_series_point(*, context, series_code, series_title, unit, value_numeric, disaggregation, footnote):
    series, _ = IndicatorDataSeries.objects.update_or_create(
        series_code=series_code,
        defaults={
            "indicator": context.indicator,
            "title": series_title,
            "unit": unit,
            "value_type": IndicatorValueType.NUMERIC,
            "methodology": "Computed from NBMS registry gold marts.",
            "disaggregation_schema": {"bucket": {"type": "string"}},
            "source_notes": "Generated by registry-derived indicator method.",
            "organisation": context.indicator.organisation,
            "status": LifecycleStatus.PUBLISHED,
            "sensitivity": SensitivityLevel.PUBLIC,
            "export_approved": True,
        },
    )
    IndicatorDataPoint.objects.update_or_create(
        series=series,
        year=_resolve_run_year(context),
        disaggregation=disaggregation,
        defaults={
            "value_numeric": value_numeric,
            "value_text": "",
            "source_url": "",
            "footnote": footnote,
        },
    )
    return series


class BinaryQuestionnaireMethod(BaseIndicatorMethod):
    key = "binary_questionnaire_aggregator"

    def run(self, context):
        framework_indicator_ids = list(
            IndicatorFrameworkIndicatorLink.objects.filter(indicator=context.indicator, is_active=True).values_list(
                "framework_indicator_id",
                flat=True,
            )
        )
        if not framework_indicator_ids:
            return MethodResult(
                status="blocked",
                output={"reason": "No framework indicator mapping for binary aggregation."},
                error_message="Indicator has no framework indicator mapping.",
            )

        responses = BinaryIndicatorResponse.objects.filter(
            question__framework_indicator_id__in=framework_indicator_ids
        ).select_related("question")
        total = responses.count()
        completed = 0
        for item in responses:
            value = item.response
            if value in (True, False):
                completed += 1
            elif isinstance(value, dict) and value:
                completed += 1
            elif isinstance(value, list) and value:
                completed += 1
        completion_pct = round((completed / total) * 100, 2) if total else 0.0
        state = "ready" if completion_pct >= 80 else ("partial" if completion_pct > 0 else "blocked")
        return MethodResult(
            status="succeeded",
            output={
                "method": self.key,
                "response_count": total,
                "completed_response_count": completed,
                "completion_percent": completion_pct,
                "progress_state": state,
            },
        )


class CsvAggregationMethod(BaseIndicatorMethod):
    key = "csv_import_aggregation"

    def run(self, context):
        points_qs = IndicatorDataPoint.objects.filter(series__indicator=context.indicator).order_by("year", "id")
        if not points_qs.exists():
            return MethodResult(
                status="blocked",
                output={"reason": "No datapoints available for CSV aggregation."},
                error_message="No indicator datapoints found.",
            )
        by_year = (
            points_qs.values("year")
            .annotate(mean_numeric=Avg("value_numeric"))
            .order_by("year")
        )
        output_rows = [
            {"year": row["year"], "mean_numeric": float(row["mean_numeric"]) if row["mean_numeric"] is not None else None}
            for row in by_year
        ]
        return MethodResult(
            status="succeeded",
            output={
                "method": self.key,
                "row_count": len(output_rows),
                "aggregated_by_year": output_rows,
            },
        )


class SpatialOverlayMethod(BaseIndicatorMethod):
    key = "spatial_overlay_area_by_province"

    def run(self, context):
        admin_layer_codes = context.params.get("admin_layer_codes") or ["ZA_PROVINCES_NE", "ZA_PROVINCES"]
        overlay_layer_codes = context.params.get("overlay_layer_codes") or [
            "ZA_PROTECTED_AREAS_NE",
            "ZA_PROTECTED_AREAS",
        ]
        admin_layer = SpatialLayer.objects.filter(layer_code__in=admin_layer_codes, is_active=True).order_by("id").first()
        overlay_layer = SpatialLayer.objects.filter(layer_code__in=overlay_layer_codes, is_active=True).order_by("id").first()
        if not admin_layer or not overlay_layer:
            return MethodResult(
                status="blocked",
                output={
                    "reason": "Required spatial layers are missing.",
                    "admin_layer_found": bool(admin_layer),
                    "overlay_layer_found": bool(overlay_layer),
                },
                error_message="Required spatial layers are missing for overlay computation.",
            )

        province_rows = []
        if GIS_ENABLED:
            with connection.cursor() as cursor:  # pragma: no cover
                cursor.execute(
                    """
                    WITH admin AS (
                        SELECT
                            sf.feature_key,
                            COALESCE(NULLIF(sf.province_code, ''), sf.feature_key, sf.feature_id, 'UNKNOWN') AS province_code,
                            COALESCE(NULLIF(sf.name, ''), sf.feature_key, sf.feature_id, 'Unknown') AS province_name,
                            COALESCE(sf.geom, ST_SetSRID(ST_GeomFromGeoJSON(sf.geometry_json::text), 4326)) AS geom
                        FROM nbms_app_spatialfeature sf
                        WHERE sf.layer_id = %s
                    ),
                    overlay_union AS (
                        SELECT ST_UnaryUnion(ST_Collect(COALESCE(sf.geom, ST_SetSRID(ST_GeomFromGeoJSON(sf.geometry_json::text), 4326)))) AS geom
                        FROM nbms_app_spatialfeature sf
                        WHERE sf.layer_id = %s
                    )
                    SELECT
                        a.province_code,
                        a.province_name,
                        ROUND((ST_Area(a.geom::geography) / 1000000.0)::numeric, 6) AS province_area_km2,
                        ROUND(
                            (
                                CASE
                                    WHEN o.geom IS NULL THEN 0
                                    ELSE ST_Area(ST_Intersection(a.geom, o.geom)::geography) / 1000000.0
                                END
                            )::numeric,
                            6
                        ) AS protected_area_km2
                    FROM admin a
                    CROSS JOIN overlay_union o
                    WHERE a.geom IS NOT NULL
                    ORDER BY a.province_code
                    """,
                    [admin_layer.id, overlay_layer.id],
                )
                province_rows = cursor.fetchall()
        else:
            admin_features = SpatialFeature.objects.filter(layer=admin_layer).order_by("province_code", "feature_key")
            overlay_features = SpatialFeature.objects.filter(layer=overlay_layer).order_by("province_code", "feature_key")
            overlay_by_province = {}
            for item in overlay_features:
                props = item.properties or item.properties_json or {}
                area = props.get("area_km2") or props.get("area_ha")
                if area is None:
                    continue
                area_km2 = float(area) if "km2" in props else float(area) / 100.0
                key = item.province_code or "UNKNOWN"
                overlay_by_province[key] = overlay_by_province.get(key, 0.0) + area_km2
            for item in admin_features:
                props = item.properties or item.properties_json or {}
                province_area = float(props.get("area_km2") or 0.0)
                key = item.province_code or "UNKNOWN"
                province_rows.append((key, item.name or key, province_area, overlay_by_province.get(key, 0.0)))

        if not province_rows:
            return MethodResult(
                status="blocked",
                output={"reason": "No overlay rows generated from current spatial layers."},
                error_message="No overlay rows generated from current spatial layers.",
            )

        province_unit_type = SpatialUnitType.objects.filter(code="PROVINCE").first()
        unit_map = {
            row.unit_code: row
            for row in SpatialUnit.objects.filter(unit_type=province_unit_type).order_by("unit_code", "id")
        } if province_unit_type else {}
        now_year = int(context.params.get("year") or timezone.now().year)
        programme_run_uuid = str(context.params.get("programme_run") or "").strip()
        programme_run = None
        if programme_run_uuid:
            programme_run = MonitoringProgrammeRun.objects.filter(uuid=programme_run_uuid).first()
        dataset_release = (
            DatasetRelease.objects.filter(
                dataset__indicator_links__indicator=context.indicator
            )
            .order_by("-release_date", "-id")
            .first()
        )

        series_code = context.params.get("series_code") or f"SER-{context.indicator.code}-SPATIAL-PA-COVERAGE"
        series, _ = IndicatorDataSeries.objects.update_or_create(
            indicator=context.indicator,
            defaults={
                "series_code": series_code,
                "title": f"{context.indicator.title} (spatial overlay by province)",
                "unit": "%",
                "value_type": IndicatorValueType.NUMERIC,
                "methodology": "Spatial overlay intersection between administrative units and protected areas.",
                "disaggregation_schema": {"province_code": {"type": "string"}, "province_name": {"type": "string"}},
                "source_notes": "Computed by spatial_overlay_area_by_province indicator method.",
                "organisation": context.indicator.organisation,
                "status": LifecycleStatus.PUBLISHED,
                "sensitivity": SensitivityLevel.PUBLIC,
                "export_approved": True,
                "spatial_unit_type": province_unit_type,
                "spatial_layer": overlay_layer,
                "spatial_resolution": "province",
            },
        )

        output_rows = []
        for province_code, province_name, area_km2, protected_km2 in province_rows:
            area_value = float(area_km2 or 0.0)
            protected_value = float(protected_km2 or 0.0)
            coverage_pct = round((protected_value / area_value) * 100.0, 6) if area_value > 0 else 0.0
            unit = unit_map.get(province_code)
            IndicatorDataPoint.objects.update_or_create(
                series=series,
                year=now_year,
                disaggregation={"province_code": province_code, "province_name": province_name},
                spatial_unit=unit,
                spatial_layer=overlay_layer,
                defaults={
                    "value_numeric": coverage_pct,
                    "value_text": "",
                    "spatial_resolution": "province",
                    "dataset_release": dataset_release,
                    "programme_run": programme_run,
                    "source_url": "",
                    "footnote": "Computed via NBMS spatial overlay method.",
                },
            )
            output_rows.append(
                {
                    "province_code": province_code,
                    "province_name": province_name,
                    "province_area_km2": round(area_value, 6),
                    "protected_area_km2": round(protected_value, 6),
                    "coverage_percent": coverage_pct,
                }
            )

        return MethodResult(
            status="succeeded",
            output={
                "method": self.key,
                "indicator_code": context.indicator.code,
                "admin_layer_code": admin_layer.layer_code,
                "overlay_layer_code": overlay_layer.layer_code,
                "series_uuid": str(series.uuid),
                "year": now_year,
                "programme_run_uuid": str(programme_run.uuid) if programme_run else None,
                "dataset_release_uuid": str(dataset_release.uuid) if dataset_release else None,
                "province_coverage": output_rows,
            },
        )


class BirdieApiConnectorMethod(BaseIndicatorMethod):
    key = "birdie_api_connector"

    def run(self, context):
        points_qs = IndicatorDataPoint.objects.filter(
            series__indicator=context.indicator,
            value_numeric__isnull=False,
        ).order_by("year", "id")
        if not points_qs.exists():
            return MethodResult(
                status="blocked",
                output={"reason": "No BIRDIE datapoints available for indicator."},
                error_message="No BIRDIE datapoints available.",
            )
        latest_two = list(points_qs.order_by("-year", "-id")[:2])
        latest = latest_two[0] if latest_two else None
        previous = latest_two[1] if len(latest_two) > 1 else None
        trend = "flat"
        if previous and latest and latest.value_numeric is not None and previous.value_numeric is not None:
            if latest.value_numeric > previous.value_numeric:
                trend = "up"
            elif latest.value_numeric < previous.value_numeric:
                trend = "down"
        return MethodResult(
            status="succeeded",
            output={
                "method": self.key,
                "point_count": points_qs.count(),
                "latest_year": latest.year if latest else None,
                "latest_value": float(latest.value_numeric) if latest and latest.value_numeric is not None else None,
                "trend": trend,
            },
        )


class EcosystemRegistrySummaryMethod(BaseIndicatorMethod):
    key = "ecosystem_registry_summary"

    def run(self, context):
        snapshot = (
            EcosystemGoldSummary.objects.order_by("-snapshot_date").values_list("snapshot_date", flat=True).first()
        )
        if not snapshot:
            return MethodResult(
                status="blocked",
                output={"reason": "No ecosystem registry gold summary snapshot available."},
                error_message="No ecosystem registry gold summary snapshot available.",
            )
        dimension = (context.params.get("dimension") or EcosystemGoldDimension.PROVINCE).strip()
        rows = list(
            EcosystemGoldSummary.objects.filter(snapshot_date=snapshot, dimension=dimension).order_by(
                "dimension_key",
                "id",
            )
        )
        if not rows:
            return MethodResult(
                status="blocked",
                output={"reason": f"No ecosystem gold rows for dimension={dimension}."},
                error_message=f"No ecosystem gold rows for dimension={dimension}.",
            )

        total_area = sum((row.total_area_km2 or Decimal("0")) for row in rows)
        protected_area = sum((row.protected_area_km2 or Decimal("0")) for row in rows)
        threatened = sum(row.threatened_count for row in rows)
        total_count = sum(row.ecosystem_count for row in rows)
        protected_pct = Decimal("0")
        threatened_pct = Decimal("0")
        if total_area > 0:
            protected_pct = (protected_area / total_area) * Decimal("100")
        if total_count > 0:
            threatened_pct = (Decimal(threatened) / Decimal(total_count)) * Decimal("100")

        series = _upsert_registry_series_point(
            context=context,
            series_code=f"SER-{context.indicator.code}-ECOSYSTEM-REGISTRY",
            series_title=f"{context.indicator.title} (ecosystem registry summary)",
            unit="percent",
            value_numeric=round(float(protected_pct), 6),
            disaggregation={
                "bucket": "national",
                "snapshot_date": snapshot.isoformat(),
                "dimension": dimension,
                "threatened_percent": round(float(threatened_pct), 6),
            },
            footnote="Computed from EcosystemGoldSummary.",
        )
        return MethodResult(
            status="succeeded",
            output={
                "method": self.key,
                "snapshot_date": snapshot.isoformat(),
                "series_uuid": str(series.uuid),
                "dimension": dimension,
                "row_count": len(rows),
                "protected_percent": round(float(protected_pct), 6),
                "threatened_percent": round(float(threatened_pct), 6),
            },
        )


class IasRegistryPressureMethod(BaseIndicatorMethod):
    key = "ias_registry_pressure_index"

    def run(self, context):
        snapshot = IASGoldSummary.objects.order_by("-snapshot_date").values_list("snapshot_date", flat=True).first()
        if not snapshot:
            return MethodResult(
                status="blocked",
                output={"reason": "No IAS registry gold summary snapshot available."},
                error_message="No IAS registry gold summary snapshot available.",
            )
        rows = list(IASGoldSummary.objects.filter(snapshot_date=snapshot).order_by("dimension", "dimension_key", "id"))
        if not rows:
            return MethodResult(
                status="blocked",
                output={"reason": "No IAS gold summary rows available."},
                error_message="No IAS gold summary rows available.",
            )
        total_profiles = sum(row.profile_count for row in rows)
        total_invasive = sum(row.invasive_count for row in rows)
        severe_categories = {EicatCategory.MR, EicatCategory.MV, SeicatCategory.MR, SeicatCategory.MV}
        severe_rows = [
            row for row in rows if row.eicat_category in severe_categories or row.seicat_category in severe_categories
        ]
        severe_count = sum(row.invasive_count for row in severe_rows)
        pressure_index = 0.0
        severe_ratio = 0.0
        if total_profiles > 0:
            pressure_index = round((total_invasive / total_profiles) * 100.0, 6)
            severe_ratio = round((severe_count / total_profiles) * 100.0, 6)

        series = _upsert_registry_series_point(
            context=context,
            series_code=f"SER-{context.indicator.code}-IAS-REGISTRY",
            series_title=f"{context.indicator.title} (IAS registry pressure)",
            unit="percent",
            value_numeric=pressure_index,
            disaggregation={
                "bucket": "national",
                "snapshot_date": snapshot.isoformat(),
                "severe_pressure_percent": severe_ratio,
            },
            footnote="Computed from IASGoldSummary.",
        )
        return MethodResult(
            status="succeeded",
            output={
                "method": self.key,
                "snapshot_date": snapshot.isoformat(),
                "series_uuid": str(series.uuid),
                "profile_count": total_profiles,
                "invasive_count": total_invasive,
                "pressure_index": pressure_index,
                "severe_pressure_percent": severe_ratio,
            },
        )


class TaxonRegistryPopulationMethod(BaseIndicatorMethod):
    key = "taxon_registry_native_voucher_ratio"

    def run(self, context):
        snapshot = TaxonGoldSummary.objects.order_by("-snapshot_date").values_list("snapshot_date", flat=True).first()
        if not snapshot:
            return MethodResult(
                status="blocked",
                output={"reason": "No taxon registry gold summary snapshot available."},
                error_message="No taxon registry gold summary snapshot available.",
            )
        rows = list(TaxonGoldSummary.objects.filter(snapshot_date=snapshot).order_by("taxon_rank", "id"))
        if not rows:
            return MethodResult(
                status="blocked",
                output={"reason": "No taxon gold summary rows available."},
                error_message="No taxon gold summary rows available.",
            )
        total_taxa = sum(row.taxon_count for row in rows)
        native_taxa = sum(row.taxon_count for row in rows if row.is_native is True)
        endemic_taxa = sum(row.taxon_count for row in rows if row.is_endemic)
        voucher_taxa = sum(row.taxon_count for row in rows if row.has_voucher)
        native_ratio = round((native_taxa / total_taxa) * 100.0, 6) if total_taxa > 0 else 0.0
        voucher_ratio = round((voucher_taxa / total_taxa) * 100.0, 6) if total_taxa > 0 else 0.0

        series = _upsert_registry_series_point(
            context=context,
            series_code=f"SER-{context.indicator.code}-TAXON-REGISTRY",
            series_title=f"{context.indicator.title} (taxon registry summary)",
            unit="percent",
            value_numeric=native_ratio,
            disaggregation={
                "bucket": "national",
                "snapshot_date": snapshot.isoformat(),
                "voucher_ratio_percent": voucher_ratio,
                "endemic_taxa_count": endemic_taxa,
            },
            footnote="Computed from TaxonGoldSummary.",
        )
        return MethodResult(
            status="succeeded",
            output={
                "method": self.key,
                "snapshot_date": snapshot.isoformat(),
                "series_uuid": str(series.uuid),
                "total_taxa": total_taxa,
                "native_taxa": native_taxa,
                "endemic_taxa": endemic_taxa,
                "native_ratio_percent": native_ratio,
                "voucher_ratio_percent": voucher_ratio,
            },
        )
